# UpgradeSchema Control File (Default)
# Conversion needs the database connection details
dbDriver=com.mysql.jdbc.Driver

dbURL=PUT_YOUR_URL_HERE
dbUser=PUT_YOUR_USERNAME_HERE
dbPass=PUT_YOUR_PASSWORD_HERE

## select one conversion for content_resource table
## FileSizeResourcesConversion adds columns for the new quota query and the resource-type query (used by OSP)
## Type1BlobResourceConversion does all of that plus switching from XML serialization to binary-entity serialization

#convert.0=FileSizeResourcesConversion
#convert.0.handler.class=org.sakaiproject.content.impl.serialize.impl.conversion.FileSizeResourcesConversionHandler
#convert.0.create.migrate.table.count=3
#convert.0.create.migrate.table.0=create table content_res_fsregister ( id varchar(1024), status varchar(99) )
#convert.0.create.migrate.table.1=create unique index content_res_fsregister_id_idx on content_res_fsregister(id)
#convert.0.create.migrate.table.2=create index content_res_fsregister_status_idx on content_res_fsregister(status)
#convert.0.drop.migrate.table=drop table content_res_fsregister
#convert.0.check.migrate.table=select count(*) from content_res_fsregister where status <> 'done'
#convert.0.select.next.batch=select id from content_res_fsregister where status = 'pending' limit 100
#convert.0.complete.next.batch=update content_res_fsregister set status = 'done' where id = ?
#convert.0.mark.next.batch=update content_res_fsregister set status = 'locked' where id = ?
#convert.0.populate.migrate.table=insert into content_res_fsregister (id,status) select RESOURCE_ID, 'pending' from CONTENT_RESOURCE
#convert.0.select.record=select XML from CONTENT_RESOURCE where RESOURCE_ID = ?
#convert.0.select.validate.record=select XML from CONTENT_RESOURCE where RESOURCE_ID = ?
#convert.0.update.record=update CONTENT_RESOURCE set CONTEXT = ?, FILE_SIZE = ?, RESOURCE_TYPE_ID = ? where RESOURCE_ID = ? 
#convert.0.new.columns.names=CONTEXT,FILE_SIZE,RESOURCE_TYPE_ID
#convert.0.new.columns.types=VARCHAR(99), BIGINT, VARCHAR(255)
#convert.0.new.columns.qualifiers=default null, default null, default null
#convert.0.new.columns.add=alter table CONTENT_RESOURCE add <name> <type> <qualifier>
#convert.0.new.columns.test=show columns from CONTENT_RESOURCE like '<name>'
#convert.0.early.termination.signal=quit.txt

convert.0=Type1BlobResourceConversion
convert.0.handler.class=org.sakaiproject.content.impl.serialize.impl.conversion.Type1BlobResourcesConversionHandler
convert.0.create.migrate.table.count=3
convert.0.create.migrate.table.0=create table content_res_t1register ( id varchar(1024), status varchar(99) )
convert.0.create.migrate.table.1=create unique index content_res_t1register_id_idx on content_res_t1register(id)
convert.0.create.migrate.table.2=create index content_res_t1register_status_idx on content_res_t1register(status)
convert.0.drop.migrate.table=drop table content_res_t1register
convert.0.check.migrate.table=select count(*) from content_res_t1register  where status <> 'done'
convert.0.select.next.batch=select id from content_res_t1register where status = 'pending' limit 100
convert.0.complete.next.batch=update content_res_t1register set status = 'done' where id = ?
convert.0.mark.next.batch=update content_res_t1register set status = 'locked' where id = ?
convert.0.populate.migrate.table=insert into content_res_t1register (id,status) select RESOURCE_ID, 'pending' from CONTENT_RESOURCE where BINARY_ENTITY is NULL
convert.0.select.record=select XML from CONTENT_RESOURCE where RESOURCE_ID = ?
convert.0.select.validate.record=select BINARY_ENTITY from CONTENT_RESOURCE where RESOURCE_ID = ?
convert.0.update.record=update CONTENT_RESOURCE set CONTEXT = ?, FILE_SIZE = ?, XML = NULL, BINARY_ENTITY = ?, RESOURCE_TYPE_ID = ? where RESOURCE_ID = ? 
convert.0.new.columns.names=CONTEXT,FILE_SIZE,RESOURCE_TYPE_ID,BINARY_ENTITY
convert.0.new.columns.types=VARCHAR(99),BIGINT,VARCHAR(255),BLOB
convert.0.new.columns.qualifiers=default null,default null,default null,default null  
convert.0.new.columns.add=alter table CONTENT_RESOURCE add <name> <type> <qualifier>
convert.0.new.columns.test=show columns from CONTENT_RESOURCE like '<name>'
convert.0.early.termination.signal=quit.txt

## convert to binary-entity serialization for content_collection table
## comment out this step unless you are switching from XML to binary-entity serialization

convert.1=Type1BlobCollectionConversion
convert.1.handler.class=org.sakaiproject.content.impl.serialize.impl.conversion.Type1BlobCollectionConversionHandler
convert.1.create.migrate.table.count=3
convert.1.create.migrate.table.0=create table content_col_t1register ( id varchar(1024), status varchar(99) )
convert.1.create.migrate.table.1=create unique index content_col_t1register_id_idx on content_col_t1register(id)
convert.1.create.migrate.table.2=create index content_col_t1register_status_idx on content_col_t1register(status)
convert.1.drop.migrate.table=drop table content_col_t1register
convert.1.check.migrate.table=select count(*) from content_col_t1register  where status <> 'done'
convert.1.select.next.batch=select id from content_col_t1register where status = 'pending' limit 100
convert.1.complete.next.batch=update content_col_t1register set status = 'done' where id = ?
convert.1.mark.next.batch=update content_col_t1register set status = 'locked' where id = ?
convert.1.populate.migrate.table=insert into content_col_t1register (id,status) select COLLECTION_ID, 'pending' from CONTENT_COLLECTION where BINARY_ENTITY IS NULL
convert.1.select.record=select XML from CONTENT_COLLECTION where COLLECTION_ID = ?
convert.1.select.validate.record=select BINARY_ENTITY from CONTENT_COLLECTION where COLLECTION_ID = ?
convert.1.update.record=update CONTENT_COLLECTION set XML = NULL, BINARY_ENTITY = ?  where COLLECTION_ID = ?
convert.1.new.columns.names=BINARY_ENTITY
convert.1.new.columns.types=BLOB
convert.1.new.columns.qualifiers=default null
convert.1.new.columns.add=alter table CONTENT_COLLECTION add <name> <type> <qualifier>
convert.1.new.columns.test=show columns from CONTENT_COLLECTION like '<name>'
convert.1.early.termination.signal=quit.txt
